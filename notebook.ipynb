{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import DSN\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted: 147\n",
      "Frames per second: 2\n",
      "Output video saved to: ./frames\\output_video.mp4\n",
      "Extracted frame paths: ['./frames\\\\frame_0001.png', './frames\\\\frame_0002.png', './frames\\\\frame_0003.png', './frames\\\\frame_0004.png', './frames\\\\frame_0005.png', './frames\\\\frame_0006.png', './frames\\\\frame_0007.png', './frames\\\\frame_0008.png', './frames\\\\frame_0009.png', './frames\\\\frame_0010.png', './frames\\\\frame_0011.png', './frames\\\\frame_0012.png', './frames\\\\frame_0013.png', './frames\\\\frame_0014.png', './frames\\\\frame_0015.png', './frames\\\\frame_0016.png', './frames\\\\frame_0017.png', './frames\\\\frame_0018.png', './frames\\\\frame_0019.png', './frames\\\\frame_0020.png', './frames\\\\frame_0021.png', './frames\\\\frame_0022.png', './frames\\\\frame_0023.png', './frames\\\\frame_0024.png', './frames\\\\frame_0025.png', './frames\\\\frame_0026.png', './frames\\\\frame_0027.png', './frames\\\\frame_0028.png', './frames\\\\frame_0029.png', './frames\\\\frame_0030.png', './frames\\\\frame_0031.png', './frames\\\\frame_0032.png', './frames\\\\frame_0033.png', './frames\\\\frame_0034.png', './frames\\\\frame_0035.png', './frames\\\\frame_0036.png', './frames\\\\frame_0037.png', './frames\\\\frame_0038.png', './frames\\\\frame_0039.png', './frames\\\\frame_0040.png', './frames\\\\frame_0041.png', './frames\\\\frame_0042.png', './frames\\\\frame_0043.png', './frames\\\\frame_0044.png', './frames\\\\frame_0045.png', './frames\\\\frame_0046.png', './frames\\\\frame_0047.png', './frames\\\\frame_0048.png', './frames\\\\frame_0049.png', './frames\\\\frame_0050.png', './frames\\\\frame_0051.png', './frames\\\\frame_0052.png', './frames\\\\frame_0053.png', './frames\\\\frame_0054.png', './frames\\\\frame_0055.png', './frames\\\\frame_0056.png', './frames\\\\frame_0057.png', './frames\\\\frame_0058.png', './frames\\\\frame_0059.png', './frames\\\\frame_0060.png', './frames\\\\frame_0061.png', './frames\\\\frame_0062.png', './frames\\\\frame_0063.png', './frames\\\\frame_0064.png', './frames\\\\frame_0065.png', './frames\\\\frame_0066.png', './frames\\\\frame_0067.png', './frames\\\\frame_0068.png', './frames\\\\frame_0069.png', './frames\\\\frame_0070.png', './frames\\\\frame_0071.png', './frames\\\\frame_0072.png', './frames\\\\frame_0073.png', './frames\\\\frame_0074.png', './frames\\\\frame_0075.png', './frames\\\\frame_0076.png', './frames\\\\frame_0077.png', './frames\\\\frame_0078.png', './frames\\\\frame_0079.png', './frames\\\\frame_0080.png', './frames\\\\frame_0081.png', './frames\\\\frame_0082.png', './frames\\\\frame_0083.png', './frames\\\\frame_0084.png', './frames\\\\frame_0085.png', './frames\\\\frame_0086.png', './frames\\\\frame_0087.png', './frames\\\\frame_0088.png', './frames\\\\frame_0089.png', './frames\\\\frame_0090.png', './frames\\\\frame_0091.png', './frames\\\\frame_0092.png', './frames\\\\frame_0093.png', './frames\\\\frame_0094.png', './frames\\\\frame_0095.png', './frames\\\\frame_0096.png', './frames\\\\frame_0097.png', './frames\\\\frame_0098.png', './frames\\\\frame_0099.png', './frames\\\\frame_0100.png', './frames\\\\frame_0101.png', './frames\\\\frame_0102.png', './frames\\\\frame_0103.png', './frames\\\\frame_0104.png', './frames\\\\frame_0105.png', './frames\\\\frame_0106.png', './frames\\\\frame_0107.png', './frames\\\\frame_0108.png', './frames\\\\frame_0109.png', './frames\\\\frame_0110.png', './frames\\\\frame_0111.png', './frames\\\\frame_0112.png', './frames\\\\frame_0113.png', './frames\\\\frame_0114.png', './frames\\\\frame_0115.png', './frames\\\\frame_0116.png', './frames\\\\frame_0117.png', './frames\\\\frame_0118.png', './frames\\\\frame_0119.png', './frames\\\\frame_0120.png', './frames\\\\frame_0121.png', './frames\\\\frame_0122.png', './frames\\\\frame_0123.png', './frames\\\\frame_0124.png', './frames\\\\frame_0125.png', './frames\\\\frame_0126.png', './frames\\\\frame_0127.png', './frames\\\\frame_0128.png', './frames\\\\frame_0129.png', './frames\\\\frame_0130.png', './frames\\\\frame_0131.png', './frames\\\\frame_0132.png', './frames\\\\frame_0133.png', './frames\\\\frame_0134.png', './frames\\\\frame_0135.png', './frames\\\\frame_0136.png', './frames\\\\frame_0137.png', './frames\\\\frame_0138.png', './frames\\\\frame_0139.png', './frames\\\\frame_0140.png', './frames\\\\frame_0141.png', './frames\\\\frame_0142.png', './frames\\\\frame_0143.png', './frames\\\\frame_0144.png', './frames\\\\frame_0145.png', './frames\\\\frame_0146.png', './frames\\\\frame_0147.png']\n"
     ]
    }
   ],
   "source": [
    "# Function to extract frames at a specified frame rate and append paths to a list\n",
    "def extract_frames(video_path, output_folder, frame_rate=2):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(3))  # Get the width of the frames\n",
    "    frame_height = int(cap.get(4))  # Get the height of the frames\n",
    "\n",
    "    # Define the codec and create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change the codec as needed\n",
    "    output_path = os.path.join(output_folder, \"output_video.mp4\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    frames = []  # List to store frame paths\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= 1.0 / frame_rate:\n",
    "            out.write(frame)\n",
    "            frame_count += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Save the frame as an image file\n",
    "            frame_filename = f\"frame_{frame_count:04d}.png\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            frames.append(frame_path)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Frames extracted: {frame_count}\")\n",
    "    print(f\"Frames per second: {frame_rate}\")\n",
    "    print(f\"Output video saved to: {output_path}\")\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "# Example usage\n",
    "video_path = \"./video/IronMan.mp4\"\n",
    "output_folder = \"./frames\"\n",
    "frames = extract_frames(video_path, output_folder, frame_rate=2)\n",
    "\n",
    "# Now 'extracted_frame_paths' contains a list of file paths for the extracted frames\n",
    "print(\"Extracted frame paths:\", frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "def get_features(frames, gpu=True, batch_size=1):\n",
    "    # Load pre-trained GoogLeNet model\n",
    "    googlenet = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', weights='GoogLeNet_Weights.DEFAULT')\n",
    "\n",
    "    # Remove the classification layer (last layer) to obtain features\n",
    "    googlenet = torch.nn.Sequential(*(list(googlenet.children())[:-1]))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    googlenet.eval()\n",
    "\n",
    "    # Initialize a list to store the features\n",
    "    features = []\n",
    "\n",
    "    # Image preprocessing pipeline\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Iterate through frames\n",
    "    for frame_path in frames:\n",
    "        # Load and preprocess the frame\n",
    "        input_image = Image.open(frame_path)\n",
    "        input_tensor = preprocess(input_image)\n",
    "        input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Move the input and model to GPU if available\n",
    "        if gpu:\n",
    "            input_batch = input_batch.to('cuda')\n",
    "            googlenet.to('cuda')\n",
    "\n",
    "        # Perform feature extraction\n",
    "        with torch.no_grad():\n",
    "            output = googlenet(input_batch)\n",
    "\n",
    "        # Append the features to the list\n",
    "        features.append(output.squeeze().cpu().numpy())\n",
    "\n",
    "    # Convert the list of features to a NumPy array\n",
    "    features = np.array(features)\n",
    "\n",
    "    return features.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def _get_probs(features, gpu=True, mode=0):\n",
    "    model_cache_key = \"keyframes_rl_model_cache_\" + str(mode)\n",
    "\n",
    "    if mode == 1:\n",
    "        model_path = \"pretrained_model/model_1.pth.tar\"\n",
    "    else:\n",
    "        model_path = \"pretrained_model/model_0.pth.tar\"\n",
    "    model = DSN(in_dim=1024, hid_dim=256, num_layers=1, cell=\"lstm\")\n",
    "    if gpu:\n",
    "        checkpoint = torch.load(model_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    if gpu:\n",
    "        model = nn.DataParallel(model).cuda()\n",
    "    model.eval()\n",
    "\n",
    "    seq = torch.from_numpy(features).unsqueeze(0)\n",
    "    if gpu: seq = seq.cuda()\n",
    "    probs = model(seq)\n",
    "    probs = probs.data.cpu().squeeze().numpy()\n",
    "    return probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Reuben/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0952245e-04 1.8711932e-02 7.0364801e-03 ... 5.4625672e-04\n",
      "  2.8798267e-01 1.9751182e-01]\n",
      " [1.0839297e-02 1.0916159e-02 6.6831982e-04 ... 4.6322305e-02\n",
      "  4.0471101e-01 1.5894957e-01]\n",
      " [7.7791312e-03 5.4033078e-02 7.6056413e-02 ... 1.7353630e-01\n",
      "  1.2400125e-01 1.4495048e-01]\n",
      " ...\n",
      " [2.8588656e-01 2.4446552e-01 6.7060399e-01 ... 1.4680484e-01\n",
      "  4.4968671e-01 9.6843272e-02]\n",
      " [2.2305962e-01 2.5735629e-01 7.4045902e-01 ... 1.6381127e-01\n",
      "  4.6176362e-01 5.7618324e-02]\n",
      " [6.8690598e-02 6.9386996e-02 0.0000000e+00 ... 6.2626995e-02\n",
      "  3.4741578e-01 2.0790750e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Reuben/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91974765 0.9454341  0.94769996 0.9487048  0.95064247 0.95125735\n",
      " 0.9565242  0.95543385 0.96018016 0.9623291  0.95793533 0.96072626\n",
      " 0.96209913 0.9584297  0.95706403 0.95564145 0.96358347 0.96281374\n",
      " 0.9595215  0.96077126 0.9604719  0.96107    0.9631843  0.9648292\n",
      " 0.9646958  0.96339834 0.95486397 0.9596439  0.9600251  0.94932085\n",
      " 0.96235466 0.9643711  0.96237177 0.95918226 0.9627097  0.96227163\n",
      " 0.9593105  0.96155834 0.9647484  0.964222   0.9642918  0.9636387\n",
      " 0.9598568  0.9552381  0.95258725 0.95290464 0.9573713  0.95899355\n",
      " 0.9637703  0.96517587 0.9582592  0.9601232  0.96382606 0.9655748\n",
      " 0.9603574  0.954497   0.96217    0.9609844  0.9618536  0.96171314\n",
      " 0.95540655 0.9560038  0.95973194 0.956878   0.95507306 0.95795316\n",
      " 0.95752627 0.9582264  0.95794547 0.9604366  0.9658313  0.96419054\n",
      " 0.9691056  0.97104025 0.9584021  0.96912795 0.970009   0.9699702\n",
      " 0.96600413 0.9680854  0.96873647 0.9695687  0.9630137  0.9605995\n",
      " 0.9634725  0.9625286  0.96550447 0.9658135  0.9610188  0.96480966\n",
      " 0.96373916 0.96681887 0.96648103 0.95950395 0.9593726  0.96208405\n",
      " 0.9611257  0.96505207 0.9649846  0.9663432  0.9603307  0.95525455\n",
      " 0.9516883  0.95664316 0.96107143 0.9622551  0.95863825 0.96426886\n",
      " 0.9606238  0.96262646 0.9628018  0.9627401  0.9601717  0.9590709\n",
      " 0.95509857 0.94751394 0.9549575  0.96399397 0.9678276  0.97345567\n",
      " 0.97429186 0.9728608  0.9701404  0.9708165  0.9687405  0.96697104\n",
      " 0.9657568  0.9671898  0.96669245 0.9688066  0.9675369  0.96263874\n",
      " 0.9665276  0.96643496 0.9665521  0.953898   0.961844   0.96206605\n",
      " 0.9527107  0.94666237 0.9547366  0.9622239  0.962248   0.9594765\n",
      " 0.95244575 0.93397605 0.8527015 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Reuben/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 1024)\n",
      "(1024,)\n",
      "(147,)\n"
     ]
    }
   ],
   "source": [
    "print(_get_features(frames))\n",
    "print(_get_probs(_get_features(frames)))\n",
    "\n",
    "features = _get_features(frames)\n",
    "print(features.shape)\n",
    "print(features[0].shape)\n",
    "print(_get_probs(features).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
